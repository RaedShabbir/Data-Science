{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: analyze sentiment and build a language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing \n",
    "\n",
    "Goal is to put data into csv format such that no headers, no index, coluns are label (0,1) and text to the right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some releveant paths and variables \n",
    "\n",
    "BOS = \"xbos\" #sentence start \n",
    "FLD = \"fld\" #data field tag \n",
    "\n",
    "PATH = Path(\"../../Data/imdb/aclImdb\")\n",
    "CLASS_PATH = Path(\"../../Data/imdb/imdb_class/\") #sentiment model \n",
    "LM_PATH = Path(\"../../Data/imdb/imdb_lm\") #language model \n",
    "\n",
    "CLASS_PATH.mkdir(exist_ok=True)\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the possible classes (positve, negative, unsupervised (posorneg dontknow!))\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "#get a list containing all trn_txts and val_texts, as well as the labels for those texts \n",
    "def get_texts(path):\n",
    "    c = 0\n",
    "    texts, labels = [], []\n",
    "    for idx, label in enumerate(CLASSES):\n",
    "        for fname in (path/label).glob(\"*.*\"):\n",
    "            c += 1\n",
    "            texts.append(fname.open('r', encoding=\"ANSI\").read())\n",
    "            labels.append(idx) \n",
    "            if c%1000 == 0 :\n",
    "                print (c)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "trn_texts, trn_labels = get_texts(PATH/'train')\n",
    "val_texts, val_labels = get_texts(PATH/'test')\n",
    "\n",
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the lists and randomize order for both sets \n",
    "np.random.seed(42)\n",
    "trn_idx =  np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))\n",
    "\n",
    "trn_texts = trn_texts[trn_idx]\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "\n",
    "val_texts = val_texts[val_idx]\n",
    "val_labels = val_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg\\n', 'pos\\n', 'unsup\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create pandas dataframes\n",
    "col_names = ['label', 'text']\n",
    "df_trn = pd.DataFrame({'text': trn_texts, 'label': trn_labels}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text': val_texts, 'label': val_labels}, columns=col_names)\n",
    "\n",
    "#create csvs from dataframes, don't include unsup images!\n",
    "df_trn[df_trn['label'] != 2].to_csv(CLASS_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(CLASS_PATH/'val.csv', header=False, index=False)\n",
    "\n",
    "#classes.txt to list the classes, each on a new line \n",
    "(CLASS_PATH/'classes.txt').open('w').writelines(f'{o}\\n' for o in CLASSES)\n",
    "(CLASS_PATH/'classes.txt').open('r').readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data into 90% and 10%\n",
    "trn_split, val_split = sklearn.model_selection.train_test_split(np.concatenate([trn_texts, val_texts]), test_size=0.1)\n",
    "len(trn_split), len(val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To keep format consistent, just have a column of 0s for label \n",
    "df_trn = pd.DataFrame({'text':trn_split, 'label':[0]*len(trn_split)}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_split, 'label':[0]*len(val_split)}, columns=col_names)\n",
    "\n",
    "df_trn.to_csv(LM_PATH/'train.csv', index=False, header=False)\n",
    "df_val.to_csv(LM_PATH/'val.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Numericalizing Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 24000 #lets pandas return a generator that lets us iterate over chunks of data frame \n",
    "\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "#remove all the strange stuff from our txt \n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    \"\"\"\n",
    "    returns tokenized text and labels for a chunk of the entire dataset\n",
    "    can recieve datasets with multiple label columns \n",
    "    \"\"\"\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)):\n",
    "        #if there are additional columns for each text (i.e. intro, concl)\n",
    "        texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "    \n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], [] \n",
    "    for i, r in enumerate(df): #loop over each chunk\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r,n_lbls)\n",
    "        tok += tok_\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(LM_PATH/'val.csv', header=None, chunksize=chunksize)\n",
    "\n",
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH/'tmp').mkdir(exist_ok=True)\n",
    "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1208310),\n",
       " ('.', 992858),\n",
       " (',', 986179),\n",
       " ('and', 588051),\n",
       " ('a', 584103),\n",
       " ('of', 524990),\n",
       " ('to', 485562),\n",
       " ('is', 393480),\n",
       " ('it', 341658),\n",
       " ('in', 337461),\n",
       " ('i', 308662),\n",
       " ('this', 270704),\n",
       " ('that', 261284),\n",
       " ('\"', 237440),\n",
       " (\"'s\", 221400),\n",
       " ('-', 188072),\n",
       " ('was', 180574),\n",
       " ('\\n\\n', 179647),\n",
       " ('as', 165776),\n",
       " ('with', 159485),\n",
       " ('for', 159023),\n",
       " ('movie', 157898),\n",
       " ('but', 150419),\n",
       " ('film', 144111),\n",
       " ('you', 124600)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Numericalize\n",
    "freq = Counter([c for i in tok_trn for c in i])\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#limit the size of the vocab based on min freq\n",
    "max_vocab = 60000\n",
    "min_freq = 2 \n",
    "\n",
    "#create ITOS, STOI\n",
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "\n",
    "#will return 0 if not in dict \n",
    "stoi = collections.defaultdict(lambda:0, {a:b for b,a in enumerate(itos)})\n",
    "\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize tokenized and numericalized data \n",
    "#' '.join(str(o) for o in tok_trn[0])\n",
    "#' '.join(str(o) for o in trn_lm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#must save itos otherwise we have a list of random numbers \n",
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy') \n",
    "itos = pickle.load( (LM_PATH/'tmp'/'itos.pkl').open(\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our model needs same size as pretrained \n",
    "em_sz, nh, nl = 400, 1150, 3\n",
    "\n",
    "#Path to pretrained folder and path to language model \n",
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'\n",
    "\n",
    "#returns dict with layer name and tensor/array of weights \n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc:storage)\n",
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "\n",
    "#get itos and create stoi for wikitext model itos  \n",
    "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1 , {v:o for o,v in enumerate(itos2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we must rearrange embedding weights to correspond with our itos \n",
    "new_w = np.zeros((len(itos), em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):\n",
    "    r = stoi2[w] #int value of imbd vocab in wikitext vocab \n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "\n",
    "#because of the way we do embedding dropout we need a seperate copy\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "\n",
    "#the decoder uses same weights \n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-7\n",
    "bptt = 50\n",
    "bs = 30\n",
    "opt_fn = partial(optim.Adam, betas=(0.8,0.99))\n",
    "\n",
    "t= len(np.concatenate(trn_lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, len(itos), trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace703b6205041ff8915e20e46eb3685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                       \n",
      "    0      4.755649   4.505801   0.255577  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.5058]), 0.25557697920344635]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-3\n",
    "lrs = lr\n",
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm_last_ft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm_last_ft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0f57262ea241d388527aff159d2e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 4328/16686 [14:47<42:14,  4.88it/s, loss=4.65]"
     ]
    }
   ],
   "source": [
    "learner.unfreeze()\n",
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)\n",
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save two different models becasue we will only use the rnn_enc portion \n",
    "#the decode is what makes it a language model \n",
    "learner.save('lm1')\n",
    "learner.save_encoder('lm1_enc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot_loss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Numericalizing Classification Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, \n",
    "                     chunksize=chunksize)\n",
    "df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, \n",
    "                     chunksize=chunksize)\n",
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\n",
    "np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
    "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
    "\n",
    "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
